# Chapters

## [Chapter 1: Warm-up](./01)

0. [Reversed string](./01/ex00.ts)
1. [“schooled”](./01/ex01.ts)
2. [“shoe” + “cold” = “schooled”](./01/ex02.ts)
3. [Pi](./01/ex03.ts)
4. [Atomic symbols](./01/ex04.ts)
5. [n-gram](./01/ex05.ts)
6. [Set](./01/ex06.ts)
7. [Template-based sentence generation](./01/ex07.ts)
8. [cipher text](./01/ex08.ts)
9. [Typoglycemia](./01/ex09.ts)

## [Chapter 2: UNIX Commands](./02)

10. [Line count](./02/ex10.ts)
11. [Replace tabs into spaces](./02/ex11.ts)
12. [col1.txt from the first column, col2.txt from the second column](./02/ex12.ts)
13. [Merging col1.txt and col2.txt](./02/ex13.ts)
14. [First N lines](./02/ex14.ts)
15. [Last N lines](./02/ex15.ts)
16. [Split a file into N pieces](./02/ex16.ts)
17. [Distinct strings in the first column](./02/ex17.ts)
18. [Sort lines in descending order of the third column](./02/ex18.ts)
19. [Frequency of a string in the first column in descending order](./02/ex19.ts)

## [Chapter 3: Regular Expression](./03)

20. Read JSON documents
21. Lines with category names
22. Category names
23. Section structure
24. Media references
25. Infobox
26. Remove emphasis markups
27. Remove internal links
28. Remove MediaWiki markups
29. Country flag

## Chapter 4: POS tagging

30. Reading the result
31. Verbs
32. Base forms of verbs
33. A of B
34. A B
35. Frequency of words
36. Top-ten frequent words
37. Top-ten words co-occurring with ‘Alice’
38. Histogram
39. Zipf’s law

## Chapter 5: Dependency parsing

40. Read the parse result (words)
41. Read the parse result (dependency)
42. Show root words
43. Show verb governors and noun dependents
44. Visualize dependency trees
45. Triple with subject, verb, and direct object
46. Expanding subjects and objects
47. Triple from the passive sentence
48. Extract paths from the root to nouns
49. Extract the shortest path between two nouns

## Chapter 6: Machine Learning

50. Download and Preprocess Dataset
51. Feature extraction
52. Training
53. Prediction
54. Accuracy score
55. Confusion matrix
56. Precision, recall and F1 score
57. Feature weights
58. Regularization
59. Hyper-parameter tuning

## Chapter 7: Word Vector

60. Loading word vectors
61. Word similarity
62. Top-10 most similar words
63. Analogy based on the additive composition
64. Analogy data experiment
65. Accuracy score on the analogy task
66. Evaluation on WordSimilarity-353
67. k-means clustering
68. Ward’s method clustering
69. t-SNE Visualization

## Chapter 8: Neural networks

70. Generating Features through Word Vector Summation
71. Building Single Layer Neural Network
72. Calculating loss and gradients
73. Learning with stochastic gradient descent
74. Measuring accuracy
75. Plotting loss and accuracy
76. Checkpoints
77. Mini-batches
78. Training on a GPU
79. Multilayer Neural Networks

## Chapter 9: RNN and CNN

80. Turning words into numeric IDs
81. Prediction with an RNN
82. Training with Stochastic Gradient Descent
83. Mini-batch Training, GPU Training
84. Add Pretrained Word Embeddings
85. Bi-directional RNN and Multi-layer RNN
86. Convolutional Neural Networks (CNN)
87. CNN Learning via Stochastic Gradient Descent
88. Hyper-parameter Tuning
89. Transfer Learning from a Pretrained Language Model

## Chapter 10: Machine Translation

90. Data Preprocessing
91. Training the machine translation model
92. Translating a text
93. BLEU score
94. Beam search
95. Subword
96. Plotting the learning curve
97. Hyper-parameter search
98. Domain Adaptation
99. Translation Server
